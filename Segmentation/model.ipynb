{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('image', cmap='gray')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dTf(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Conv2d with the padding behavior from TF\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=True, padding_mode='zeros'):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias, padding_mode)\n",
    "        self.padding = padding\n",
    "    \n",
    "    def _compute_padding(self, input, dim):\n",
    "        input_size = input.size(dim + 2)\n",
    "        filter_size = self.weight.size(dim + 2)\n",
    "        effective_filter_size = (filter_size - 1) * self.dilation[dim] + 1\n",
    "        out_size = (input_size + self.stride[dim] - 1) // self.stride[dim]\n",
    "        total_padding = max(\n",
    "            0, (out_size - 1) * self.stride[dim] + effective_filter_size - input_size\n",
    "        )\n",
    "        additional_padding = int(total_padding % 2 != 0)\n",
    "\n",
    "        return additional_padding, total_padding\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.padding == 'valid':\n",
    "            return F.conv2d(input, self.weight, self.bias, self.stride, 0, self.dilation, self.groups)\n",
    "        elif self.padding == 'same':\n",
    "            rows_odd, padding_rows = self._compute_padding(input, dim=0)\n",
    "            cols_odd, padding_cols = self._compute_padding(input, dim=1)\n",
    "            if rows_odd or cols_odd:\n",
    "                input = F.pad(input, [0, cols_odd, 0, rows_odd])\n",
    "            return F.conv2d(input, self.weight, self.bias, self.stride, (padding_rows//2, padding_cols//2), self.dilation, self.groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 1, 6, 8)\n",
    "x = torch.ones(input_shape, requires_grad=False)\n",
    "l = Conv2dTf(1, 1, 4, padding='same', bias=False)\n",
    "l.weight.data.fill_(1)\n",
    "y = l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9., 12., 12., 12., 12., 12.,  9.,  6.],\n",
       "       [12., 16., 16., 16., 16., 16., 12.,  8.],\n",
       "       [12., 16., 16., 16., 16., 16., 12.,  8.],\n",
       "       [12., 16., 16., 16., 16., 16., 12.,  8.],\n",
       "       [ 9., 12., 12., 12., 12., 12.,  9.,  6.],\n",
       "       [ 6.,  8.,  8.,  8.,  8.,  8.,  6.,  4.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y.detach().numpy().squeeze()\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9., 12., 12.,  9.],\n",
       "       [12., 16., 16., 12.],\n",
       "       [ 9., 12., 12.,  9.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (1, 1, 6, 8)\n",
    "x = torch.ones(input_shape, requires_grad=False)\n",
    "l = nn.Conv2d(1, 1, 4, stride=2, padding=1, bias=False)\n",
    "l.weight.data.fill_(1)\n",
    "y = l(x)\n",
    "y2 = y.detach().numpy().squeeze()\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 1., 2., 1., 2., 1., 2.],\n",
       "       [3., 4., 3., 4., 3., 4., 3., 4.],\n",
       "       [1., 2., 1., 2., 1., 2., 1., 2.],\n",
       "       [3., 4., 3., 4., 3., 4., 3., 4.],\n",
       "       [1., 2., 1., 2., 1., 2., 1., 2.],\n",
       "       [3., 4., 3., 4., 3., 4., 3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1, 1, 3, 4)\n",
    "upsample = nn.ConvTranspose2d(1, 1, 2, stride=2, padding=0, bias=False)\n",
    "upsample.weight = nn.Parameter(torch.Tensor([[[[1, 2], [3, 4]]]]))\n",
    "y = upsample(x)\n",
    "y2 = y.detach().numpy().squeeze()\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 6, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepVOGEncodingBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, down_sampling=True, skip=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels*2 if down_sampling else out_channels\n",
    "        self.kernel_size = kernel_size if type(kernel_size)==tuple else (kernel_size, kernel_size)\n",
    "        self.down_sampling = down_sampling\n",
    "        self.skip = skip\n",
    "        self.add_module(f'conv_main', Conv2dTf(in_channels, out_channels, kernel_size))\n",
    "        self.add_module(f'bn_main', nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1))\n",
    "        self.add_module(f'act_main', nn.ReLU())\n",
    "        if down_sampling:\n",
    "            self.add_module(f'conv_down', nn.Conv2d(out_channels, out_channels*2, 2, stride=2))\n",
    "            self.add_module(f'bn_down', nn.BatchNorm2d(out_channels*2, eps=0.001, momentum=0.1))\n",
    "            self.add_module(f'act_down', nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self._modules['conv_main'](x)\n",
    "        x = self._modules['bn_main'](x)\n",
    "        x = self._modules['act_main'](x)\n",
    "        if self.down_sampling:\n",
    "            x_down = self._modules['conv_down'](x)\n",
    "            x_down = self._modules['bn_down'](x_down)\n",
    "            x_down = self._modules['act_down'](x_down)\n",
    "        else:\n",
    "            x_down = x\n",
    "        if self.skip:\n",
    "            return x_down, x\n",
    "        else:\n",
    "            return x_down\n",
    "\n",
    "class DeepVOGDecodingBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, jump_channels=0, up_sampling=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if type(kernel_size)==tuple else (kernel_size, kernel_size)\n",
    "        self.jump_channels = jump_channels\n",
    "        self.up_sampling = up_sampling\n",
    "        self.add_module(f'conv_main', Conv2dTf(in_channels+jump_channels, out_channels, kernel_size))\n",
    "        self.add_module(f'bn_main', nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1))\n",
    "        self.add_module(f'act_main', nn.ReLU())\n",
    "        if up_sampling:\n",
    "            self.add_module(f'conv_up', nn.ConvTranspose2d(out_channels, out_channels, 2, stride=2))\n",
    "            self.add_module(f'bn_up', nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1))\n",
    "            self.add_module(f'act_up', nn.ReLU())\n",
    "        \n",
    "    def forward(self, x, x_jump=None):\n",
    "        if self.jump_channels>0:\n",
    "            if x_jump is None:\n",
    "                raise ValueError('Jump connected defined in model but not provided.')\n",
    "            if x_jump.shape[1]!=self.jump_channels:\n",
    "                raise ValueError(f'Expected {self.jump_channels} channels in the jump connection, got {x_jump.shape[1]}')\n",
    "            x = torch.cat((x, x_jump), 1)\n",
    "        x = self._modules['conv_main'](x)\n",
    "        x = self._modules['bn_main'](x)\n",
    "        x = self._modules['act_main'](x)\n",
    "        if self.up_sampling:\n",
    "            x_up = self._modules['conv_up'](x)\n",
    "            x_up = self._modules['bn_up'](x_up)\n",
    "            x_up = self._modules['act_up'](x_up)\n",
    "        else:\n",
    "            x_up = x\n",
    "        return x_up\n",
    "\n",
    "class DeepVOG(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=3, out_channels=3, down_channels=(16, 32, 64, 128), up_channels=(256, 256, 128, 64, 32), kernel_size=(10, 10)):\n",
    "        super().__init__()\n",
    "        self.layers_down = len(down_channels)\n",
    "        self.layers_up = len(up_channels)\n",
    "        if self.layers_up!=self.layers_down+1:\n",
    "            raise ValueError('Number of upsampling layers must be greater than the number of downsampling layer by 1.')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.down_channels = down_channels\n",
    "        self.up_channels = up_channels\n",
    "        self.down_in_channels = (in_channels, *tuple([s*2 for s in down_channels[:-1]]))\n",
    "        self.down_out_channels = tuple([s*2 for s in down_channels])\n",
    "        self.up_in_channels = (self.down_out_channels[-1], *up_channels[:-1])\n",
    "        self.up_out_channels = up_channels\n",
    "        self.jump_channels = (0, *down_channels[::-1])\n",
    "        self.up_sampling = tuple([k<self.layers_up-1 for k in range(self.layers_up)])\n",
    "        for k in range(self.layers_down):\n",
    "            self.add_module(f'down_{k+1}', DeepVOGEncodingBlock(self.down_in_channels[k], self.down_out_channels[k]//2, kernel_size))\n",
    "        for k in range(self.layers_up):\n",
    "            self.add_module(f'up_{k+1}', DeepVOGDecodingBlock(self.up_in_channels[k], self.up_out_channels[k], kernel_size, self.jump_channels[k], self.up_sampling[k]))\n",
    "        self.add_module(f'conv_out', nn.Conv2d(up_channels[-1], out_channels, (1, 1)))\n",
    "        self.add_module(f'act_out', nn.Softmax2d())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.shape[1]!=self.in_channels:\n",
    "            raise ValueError(f'Expected {self.in_channels} channels in the input, got {x.shape[1]}')\n",
    "        x_skips = []\n",
    "        for k in range(self.layers_down):\n",
    "            x, x_skip = self._modules[f'down_{k+1}'](x)\n",
    "            #print(f'down_{k+1}: {x.shape}, skip_{k+1}: {x_skip.shape}')\n",
    "            x_skips.append(x_skip)\n",
    "        x_skips.append(None)\n",
    "        x_skips = x_skips[::-1]\n",
    "        for k in range(self.layers_up):\n",
    "            x = self._modules[f'up_{k+1}'](x, x_skips[k])\n",
    "            #print(f'up_{k+1}: {x.shape}')\n",
    "        x = self._modules[f'conv_out'](x)\n",
    "        x = self._modules[f'act_out'](x)\n",
    "        #print(f'out: {x.shape}')\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepVOG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['down_1.conv_main.weight', 'down_1.conv_main.bias', 'down_1.bn_main.weight', 'down_1.bn_main.bias', 'down_1.bn_main.running_mean', 'down_1.bn_main.running_var', 'down_1.bn_main.num_batches_tracked', 'down_1.conv_down.weight', 'down_1.conv_down.bias', 'down_1.bn_down.weight', 'down_1.bn_down.bias', 'down_1.bn_down.running_mean', 'down_1.bn_down.running_var', 'down_1.bn_down.num_batches_tracked', 'down_2.conv_main.weight', 'down_2.conv_main.bias', 'down_2.bn_main.weight', 'down_2.bn_main.bias', 'down_2.bn_main.running_mean', 'down_2.bn_main.running_var', 'down_2.bn_main.num_batches_tracked', 'down_2.conv_down.weight', 'down_2.conv_down.bias', 'down_2.bn_down.weight', 'down_2.bn_down.bias', 'down_2.bn_down.running_mean', 'down_2.bn_down.running_var', 'down_2.bn_down.num_batches_tracked', 'down_3.conv_main.weight', 'down_3.conv_main.bias', 'down_3.bn_main.weight', 'down_3.bn_main.bias', 'down_3.bn_main.running_mean', 'down_3.bn_main.running_var', 'down_3.bn_main.num_batches_tracked', 'down_3.conv_down.weight', 'down_3.conv_down.bias', 'down_3.bn_down.weight', 'down_3.bn_down.bias', 'down_3.bn_down.running_mean', 'down_3.bn_down.running_var', 'down_3.bn_down.num_batches_tracked', 'down_4.conv_main.weight', 'down_4.conv_main.bias', 'down_4.bn_main.weight', 'down_4.bn_main.bias', 'down_4.bn_main.running_mean', 'down_4.bn_main.running_var', 'down_4.bn_main.num_batches_tracked', 'down_4.conv_down.weight', 'down_4.conv_down.bias', 'down_4.bn_down.weight', 'down_4.bn_down.bias', 'down_4.bn_down.running_mean', 'down_4.bn_down.running_var', 'down_4.bn_down.num_batches_tracked', 'up_1.conv_main.weight', 'up_1.conv_main.bias', 'up_1.bn_main.weight', 'up_1.bn_main.bias', 'up_1.bn_main.running_mean', 'up_1.bn_main.running_var', 'up_1.bn_main.num_batches_tracked', 'up_1.conv_up.weight', 'up_1.conv_up.bias', 'up_1.bn_up.weight', 'up_1.bn_up.bias', 'up_1.bn_up.running_mean', 'up_1.bn_up.running_var', 'up_1.bn_up.num_batches_tracked', 'up_2.conv_main.weight', 'up_2.conv_main.bias', 'up_2.bn_main.weight', 'up_2.bn_main.bias', 'up_2.bn_main.running_mean', 'up_2.bn_main.running_var', 'up_2.bn_main.num_batches_tracked', 'up_2.conv_up.weight', 'up_2.conv_up.bias', 'up_2.bn_up.weight', 'up_2.bn_up.bias', 'up_2.bn_up.running_mean', 'up_2.bn_up.running_var', 'up_2.bn_up.num_batches_tracked', 'up_3.conv_main.weight', 'up_3.conv_main.bias', 'up_3.bn_main.weight', 'up_3.bn_main.bias', 'up_3.bn_main.running_mean', 'up_3.bn_main.running_var', 'up_3.bn_main.num_batches_tracked', 'up_3.conv_up.weight', 'up_3.conv_up.bias', 'up_3.bn_up.weight', 'up_3.bn_up.bias', 'up_3.bn_up.running_mean', 'up_3.bn_up.running_var', 'up_3.bn_up.num_batches_tracked', 'up_4.conv_main.weight', 'up_4.conv_main.bias', 'up_4.bn_main.weight', 'up_4.bn_main.bias', 'up_4.bn_main.running_mean', 'up_4.bn_main.running_var', 'up_4.bn_main.num_batches_tracked', 'up_4.conv_up.weight', 'up_4.conv_up.bias', 'up_4.bn_up.weight', 'up_4.bn_up.bias', 'up_4.bn_up.running_mean', 'up_4.bn_up.running_var', 'up_4.bn_up.num_batches_tracked', 'up_5.conv_main.weight', 'up_5.conv_main.bias', 'up_5.bn_main.weight', 'up_5.bn_main.bias', 'up_5.bn_main.running_mean', 'up_5.bn_main.running_var', 'up_5.bn_main.num_batches_tracked', 'conv_out.weight', 'conv_out.bias'])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 240, 320])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepVOG_net(input_shape = (240, 320, 3), filter_size= (3,3)):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    Nh, Nw = input_shape[0], input_shape[1]\n",
    "    \n",
    "    # Encoding Stream\n",
    "    X_jump1, X_out = encoding_block(X = X_input, X_skip = 0, filter_size= filter_size, filters_num= 16,\n",
    "                                      layer_num= 1, block_type = \"down\", stage = 1, s = 1)\n",
    "    X_jump2, X_out = encoding_block(X = X_out, X_skip = X_out, filter_size= filter_size, filters_num= 32,\n",
    "                                      layer_num= 1, block_type = \"down\", stage = 2, s = 1)\n",
    "    X_jump3, X_out = encoding_block(X = X_out, X_skip = X_out, filter_size= filter_size, filters_num= 64,\n",
    "                                      layer_num= 1, block_type = \"down\", stage = 3, s = 1)\n",
    "    X_jump4, X_out = encoding_block(X = X_out, X_skip = X_out, filter_size= filter_size, filters_num= 128,\n",
    "                                      layer_num= 1, block_type = \"down\", stage = 4, s = 1)\n",
    "    \n",
    "    # Decoding Stream\n",
    "    X_out = decoding_block(X = X_out, X_jump = None, filter_size= filter_size, filters_num= 256, \n",
    "                                 layer_num= 1, block_type = \"up\", stage = 1, s = 1)\n",
    "    X_out = decoding_block(X = X_out, X_jump = X_jump4, filter_size= filter_size, filters_num= 256, \n",
    "                                 layer_num= 1, block_type = \"up\", stage = 2, s = 1)\n",
    "    X_out = decoding_block(X = X_out, X_jump = X_jump3, filter_size= filter_size, filters_num= 128, \n",
    "                                 layer_num= 1, block_type = \"up\", stage = 3, s = 1)\n",
    "    X_out = decoding_block(X = X_out, X_jump = X_jump2, filter_size= filter_size, filters_num= 64, \n",
    "                                 layer_num= 1, block_type = \"up\", stage = 4, s = 1)\n",
    "    X_out = decoding_block(X = X_out, X_jump = X_jump1, filter_size= filter_size, filters_num= 32, \n",
    "                                 layer_num= 1, block_type = \"up\", stage = 5, s = 1, up_sampling = False)\n",
    "    # Output layer operations\n",
    "    X_out = Conv2D(filters = 3, kernel_size = (1,1) , strides = (1,1), padding = 'valid',\n",
    "                   name = \"conv_out\", kernel_initializer = glorot_uniform())(X_out)\n",
    "    X_out = Activation(\"softmax\")(X_out)\n",
    "    model = Model(inputs = X_input, outputs = X_out, name='Pupil')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
